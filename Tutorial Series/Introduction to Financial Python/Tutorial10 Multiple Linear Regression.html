<h2>Introduction</h2>

In the last chapter we introduced simple linear regression and model fitness test. In this chapter we will expand our independent variable from one to many, which is known as <strong>Multiple Linear Regression</strong>.

A simple linear regression model is usually written in the following form:
\[ Y = \alpha + \beta X + \epsilon \]

A multiple linear regression model with $p$ variables is given by:
\[ Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 +......+ \beta_p X_p \]

<h2>Python Implementation</h2>

In the last chapter we used S&amp;P 500 index as a variable to predict Amazon stock. Now we can add more variable to see whether the fitness of the model increases.

<pre class="prettyprint linenums">import numpy as np
import pandas as pd
import quandl
import matplotlib.pyplot as plt
import statsmodels.formula.api as sm

quandl.ApiConfig.api_key = 'tAyfv1zpWnyhmDsp91yv'
spy_table  = quandl.get('BCIW/_SPXT')
amzn_table = quandl.get('WIKI/AMZN')
ebay_table = quandl.get('WIKI/EBAY')
wal_table  = quandl.get('WIKI/WMT')
aapl_table = quandl.get('WIKI/AAPL')

spy  = spy_table .loc['2016',['Close']]
amzn = amzn_table.loc['2016',['Close']]
ebay = ebay_table.loc['2016',['Close']]
wal  = wal_table .loc['2016',['Close']]
aapl = aapl_table.loc['2016',['Close']]

spy_log  = np.log(spy.Close) .diff().dropna()
amzn_log = np.log(amzn.Close).diff().dropna()
ebay_log = np.log(ebay.Close).diff().dropna()
wal_log  = np.log(wal.Close) .diff().dropna()
aapl_log = np.log(aapl.Close).diff().dropna()

df = pd.concat([spy_log,amzn_log,ebay_log,wal_log,aapl_log],axis = 1).dropna()
df.columns = ['SPY', 'AMZN', 'EBAY', 'WAL', 'AAPL']
df.tail()
</pre>

This yields the log returns of each stock in the last 5 days:

<table>
    <tr>
        <td>Date</td>
        <td align="right">SPY</td>
        <td align="right">AMZN</td>
        <td align="right">EBAY</td>
        <td align="right">WAL</td>
        <td align="right">AAPL</td>
    </tr>
    <tr>
        <td>2016-12-23</td>
        <td align="right">0.001351</td>
        <td align="right">-0.007531</td>
        <td align="right">0.008427</td>
        <td align="right">-0.000719</td>
        <td align="right">0.001976</td>
    </tr>
    <tr>
        <td>2016-12-27</td>
        <td align="right">0.002254</td>
        <td align="right">0.014113</td>
        <td align="right">0.014993</td>
        <td align="right">0.002298</td>
        <td align="right">0.006331</td>
    </tr>
    <tr>
        <td>2016-12-28</td>
        <td align="right">-0.008218</td>
        <td align="right">0.000946</td>
        <td align="right">-0.007635</td>
        <td align="right">-0.005611</td>
        <td align="right">-0.004273</td>
    </tr>
    <tr>
        <td>2016-12-29</td>
        <td align="right">-0.000247</td>
        <td align="right">-0.009081</td>
        <td align="right">-0.001000</td>
        <td align="right">-0.000722</td>
        <td align="right">-0.000257</td>
    </tr>
    <tr>
        <td>2016-12-30</td>
        <td align="right">-0.004601</td>
        <td align="right">-0.020172</td>
        <td align="right">-0.009720</td>
        <td align="right">-0.002023</td>
        <td align="right">-0.007826</td>
    </tr>
</table>

Now let's build a model. Recall that we can perform a simple linear regression model using the 'statsmodels' package by inputing the formula:

<pre class="prettyprint linenums">simple = sm.ols(formula = 'amzn ~ spy',data = df).fit()
print simple.summary()

                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept   9.876e-05      0.001      0.097      0.923      -0.002       0.002
spy            1.0796      0.124      8.725      0.000       0.836       1.323
</pre>

Similarly, we can build a multiple linear regression model:

<pre class="prettyprint linenums">model = sm.ols(formula = 'amzn ~ spy+ebay+wal',data = df).fit()
print model.summary()

                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0001      0.001      0.134      0.894      -0.002       0.002
spy            1.0468      0.170      6.155      0.000       0.712       1.382
ebay          -0.0795      0.058     -1.364      0.174      -0.194       0.035
wal           -0.0865      0.089     -0.976      0.330      -0.261       0.088
aapl           0.1529      0.084      1.831      0.068      -0.012       0.317
</pre>

As seen from the summary table, the P-values for Ebay, Walmart and Apple are 0.174, 0.330 and 0.068 respectively, so none of the them are significant at a 95% confidence.

The multiple regression model has a higher R-square than the simple one: 0.254 vs 0.234. Indeed, R-square cannot decrease as the number of variables increases. This is because if additional variables do not explain the response (amzn), then their estimated coefficients will be zero, without losing explanatory power.

However, this doesn't mean it's better to add hundreds of variables. We will talk about overfitting in another chapter.

The reason why it's just increased a little is that we selected some stocks based on our intuition. We think that the competitors and the substitutes of AMZN may explain AMZN's daily return, but actually they don't. Here we try <strong>Fama-French 5 factor</strong>, which is an important model asset pricing theory and we will cover it in the later tutorials.

<pre class="prettyprint linenums">fama_table = quandl.get('KFRENCH/FACTORS5_D')
fama = fama_table['2016']
fama = fama.rename(columns = {'Mkt-RF':'MKT'})
fama = fama.apply(lambda x: x/100)
fama_df = pd.concat([fama,amzn_log],axis = 1)
fama_model = sm.ols(formula = 'Close~MKT+SMB+HML+RMW+CMA',data = fama_df).fit()
print fama_model.summary()
</pre>

As we can see, the r-square increased significantly if we use Fama-French 5 factor to do the multiple lienar regression. We can compare the prediction from simple linear regression and Fama-French multiple regression by plotting them together on one chart:

<pre class="prettyprint linenums">result = pd.DataFrame({'simple regression': simple.predict(),
                       'fama_french': fama_model.predict(),
                       'sample': df.amzn}, index = df.index)
plt.figure(figsize = (15,7.5))
plt.plot(result['2016-7':'2016-9'].index,result.loc['2016-7':'2016-9','simple regression'])
plt.plot(result['2016-7':'2016-9'].index,result.loc['2016-7':'2016-9','fama_french'])
plt.plot(result['2016-7':'2016-9'].index,result.loc['2016-7':'2016-9','sample'])
plt.legend()
plt.show()
</pre>

Although it's hard to observe, we can still found that the prediction from multiple regression is closer to the actual value. Usually we don't plot the result to observe which model is better, instead we read the statistical results from the summary table to make decisions.

<h2>Model Significance Test</h2>

From the last tutorial we know that r-square is an important statistical indicator for parameter significance. Now we are going to introduce another indicator for only multiple linear regression: F-score.

The null hypothesis and alternative hypothesis for F-test are:
\[H_0: \beta_1 = \beta_2 = \beta_3 =... = \beta_n = 0\]
\[ H_1: \text{At least one coefficient is not 0} \]

We won't explain F test procedure in detail here. You just need to understand the null hypothesis and alternative hypothesis. The 'F-statistic' in the summary table is the value for F-test, and the 'prob (F-statistic)' is the p-value for F-test. As you can see, the P-value of our F-test is extreme small, which means we are nearly 100% sure that at least one of the coefficient is not 0 from the statistic result.

After setting up a multiple linear regression model, the first step to do is check the F-score for model significance. If the p-value is not small enough to reject the null hypothesis, you should reconsider the feasibility of this model and find some other variables to build a new one.

If we are doing simple linear regression, the null hypothesis for F-test and the null hypothesis for t-test on the slope should be exactly the same, so the p-values for both test should also be the same. You can find it from the simple linear regression summary table above.

<h2>Residual Analysis</h2>

In linear regression, we often assume that the residuals are independent and normally distributed with the same variance (homoskedasticity), so that we can contruct prediction intervals, for example.

<h3>Normality</h3>
We can plot the density of the residual to check for normality:

<pre class="prettyprint linenums">plt.figure()
fama_model.resid.plot.density()
plt.show()
</pre>

As seen from the plot, the residual is normally distributed. By the way, the residual mean is always zero, up to machine precision:

<pre class="prettyprint linenums">print 'Residual mean: ', np.mean(fama_model.resid)
print 'Residual variance: ', np.var(fama_model.resid)
</pre>

<h3>Homoskedasticity</h3>

This word is difficult to pronounce but not difficult to understand. It means that the residuals have the same variance for all values of X. Otherwise we say 'heteroskedasticity' is detected.

<pre class="prettyprint linenums">plt.figure(figsize = (20,10))
plt.scatter(df.spy,simple.resid)
plt.axhline(0.05)
plt.axhline(-0.05)
plt.xlabel('x value')
plt.ylabel('residual')
plt.show()
</pre>

As seen from the chart, the variance of residual doesn't increase with X obviously, except for three outliers, which would not change our conclusion. We can plot to test heteroskedasticity for simple linear regression. However, we can't do this for multiple regression, we use Python package instead:

<pre class="prettyprint linenums">from statsmodels.stats import diagnostic as dia
het = dia.het_breuschpagan(fama_model.resid,fama_df[['MKT','SMB','HML','RMW','CMA']][1:])
print 'p-value of Heteroskedasticity: ', het[-1]
</pre>

For the multiple regression, we have 85% confidence level to claim that no heteroskedasticity is detected.

<h2>Summary</h2>
In this chapter we introduced multiple linear regression, F-test and basic residual analysis, which are the basic for quantitative linear financial modeling. Next chapter we will introduce some linear algebra, which are used for the model portfolio theory and CAPM.